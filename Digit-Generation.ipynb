{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Generation ðŸ”¢\n",
    "\n",
    "![](https://images.unsplash.com/photo-1502570149819-b2260483d302?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80)\n",
    "\n",
    "Photo by [Nick Hillier](https://unsplash.com/photos/yD5rv8_WzxA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A GAN to generate digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will be asked to :\n",
    "- complete the code to create your first GAN\n",
    "- train a GAN to generate digits based on the MNIST dataset\n",
    "\n",
    "You should be able to generate new digits by the end of the exercise. This exercise can be run locally, but you can also go for a notebook in Google Colab for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:28.715849Z",
     "start_time": "2019-05-30T12:24:28.706796Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 12:16:38.700335: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T16:33:17.276747Z",
     "start_time": "2019-05-29T16:33:17.272840Z"
    }
   },
   "source": [
    "## I. Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this GAN example, we're going to use the MNIST dataset. MNIST is a set of handwritten digits. We'll try to generate new digit samples using GANs.\n",
    "\n",
    "We kindly remind you how to load the data ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:30.426759Z",
     "start_time": "2019-05-30T12:24:29.727444Z"
    }
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Some useful variables\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. Rescale the data from -1 to 1 and format the X_train dataset in order to have the proper dimensions\n",
    "\n",
    "> ðŸ”¦ **Hint**: Remember, the MNIST dataset is grayscale so contains only one channel but Keras expects input images to have 3 dimensions even if there is only one channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:23:37.863536Z",
     "start_time": "2019-12-03T09:23:37.860180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Rescale -1 to 1 and format the X_train dataset\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train_rescaled = scaler.fit_transform(X_train.reshape(-1, img_rows * img_cols))\n",
    "\n",
    "X_train_sc = X_train_rescaled.reshape(-1, *img_shape)\n",
    "\n",
    "X_train_sc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**.Visualize one of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:23:51.734889Z",
     "start_time": "2019-12-03T09:23:51.730345Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '6')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO : Visualize one image\n",
    "ind = np.random.randint(len(X_train_sc))\n",
    "plt.imshow(X_train_sc[ind], cmap ='gray')\n",
    "plt.title(y_train[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T16:33:17.276747Z",
     "start_time": "2019-05-29T16:33:17.272840Z"
    }
   },
   "source": [
    "## II. Build the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:31.385068Z",
     "start_time": "2019-05-30T12:24:31.374005Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, LeakyReLU, UpSampling2D, Conv2D\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "#from keras.layers.advanced_activations import LeakyReLU\n",
    "#from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. The Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. The first step is to build a generator. For the generator, we start with an **input noise shape of size 100**. We then create a sequential model to increase the size of the data up to 1024, before reshaping the data back to the input image shape.\n",
    "\n",
    "Each layer will be made of:\n",
    "- A **Dense layer** (sizes 256, 512, 1024 in order)\n",
    "- A **LeakyRelu activation** with alpha = 0.2\n",
    "- A **Batch normalization** (momentum = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:32.487781Z",
     "start_time": "2019-05-30T12:24:32.468680Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    # Input Data\n",
    "    noise_shape = (100,)\n",
    "    noise = Input(shape=noise_shape)\n",
    "    \n",
    "    # Create the sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Build the first layer\n",
    "    model.add(Dense(256, input_shape = noise_shape))\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "    model.add(BatchNormalization(momentum =0.8))\n",
    "    \n",
    "    # Second layer\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "    model.add(BatchNormalization(momentum =0.8))\n",
    "    \n",
    "    # Third layer\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "    model.add(BatchNormalization(momentum = 0.8))\n",
    "    \n",
    "    # Flatten and reshape\n",
    "    model.add(Dense(np.prod(img_shape), activation = 'tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "\n",
    "    # Get model summary\n",
    "    img = model(noise)\n",
    "    #print(model.summary)\n",
    "    \n",
    "    return Model(noise, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**. Compile the Generator and add an Adam optimizer as advised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:25:18.312334Z",
     "start_time": "2019-12-03T09:25:18.307706Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO : Compile the generator\n",
    "generator_model = build_generator()\n",
    "generator_model.compile(optimizer = Adam(), loss = 'binary_crossentropy', metrics = ['accuracy'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. The Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3**. Build the discriminator. It takes an input that has the shape of the image. The steps are the following :\n",
    "- Declare the **Sequential** model\n",
    "- **Flatten** the images (with input shape = image shape)\n",
    "- Add a **Dense layer** of 512 and a **Leaky Relu** (0.2)\n",
    "- Add a **Dense layer** of 256 and a **Leaky Relu** (0.2)\n",
    "- Add a **Dense layer** of size 1. What activation function would you use ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:34.899788Z",
     "start_time": "2019-05-30T12:24:34.885304Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    \n",
    "    img = Input(shape=img_shape)\n",
    "\n",
    "    # Create the sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Flatten the images taken as inputs\n",
    "    model.add(Flatten(input_shape = img_shape))\n",
    "    \n",
    "    # First layer\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "    \n",
    "    # Second layer\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "    \n",
    "    # Last layer, return either 0 or 1\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    # Get model summary\n",
    "    validity = model(img)\n",
    "    \n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**. Now compile the discriminator. (Observe the metric we are using)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:25:49.184553Z",
     "start_time": "2019-12-03T09:25:49.180943Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Compile the discriminator\n",
    "discriminator_model = build_discriminator()\n",
    "discriminator_model.compile(optimizer = Adam(), loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Build the GAN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5**. Now it is time to build **the GAN model**. This is done in 4 major steps :\n",
    "- Declare the input\n",
    "- Set the image as the result of the generator of the input\n",
    "- Set the output as the result of the discriminator of the generated image\n",
    "- Define and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:26:11.801129Z",
     "start_time": "2019-12-03T09:26:11.795990Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Declare input of size (100, )\n",
    "latent_dim = 100 #z (do not call latent)\n",
    "generator_input = Input(shape=(latent_dim,))\n",
    "\n",
    "# 2. Define the generated image from the input\n",
    "# Hint : Use the generator model compiled above\n",
    "generated_image = generator_model(generator_input)\n",
    "\n",
    "# 3. Define the output from the image\n",
    "# Hint : Use the discriminator model compiled above\n",
    "output_image = discriminator_model(generated_image)\n",
    "\n",
    "# For the combined model, only train the generator\n",
    "discriminator_model.trainable = False\n",
    "\n",
    "# 4.Combined model\n",
    "# Create the model by defining the input and the output\n",
    "GAN_model = Model(generator_input, output_image)\n",
    "\n",
    "# Once created, we compile the model\n",
    "GAN_model.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6**. Print the summary of the new model created. Comment on the shapes at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:26:20.574554Z",
     "start_time": "2019-12-03T09:26:20.569620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 28, 28, 1)         1493520   \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 1)                 533505    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2027025 (7.73 MB)\n",
      "Trainable params: 1489936 (5.68 MB)\n",
      "Non-trainable params: 537089 (2.05 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print model summary\n",
    "GAN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a function that is used to save generated images once in a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:24:41.361699Z",
     "start_time": "2019-05-30T12:24:41.349843Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_imgs(epoch):\n",
    "    \n",
    "    # Predict from input noise\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, 100))\n",
    "    gen_imgs = generator_model.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "    \n",
    "    # Subplots\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    " \n",
    "    fig.savefig(\"images_gan/mnist_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we set :\n",
    "- the number of epochs the model will train to 15'000\n",
    "- the batch size to 64\n",
    "- the interval at which we save the images to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:28:50.729562Z",
     "start_time": "2019-05-30T12:28:50.722302Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 15000\n",
    "batch_size = 64\n",
    "save_interval = 1000\n",
    "half_batch = int(batch_size / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is complete. Try to understand the different steps, debug potential errors from your previous code and compile it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the directory to save the images does not exist create it \n",
    "#!mkdir images_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((half_batch, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T12:51:49.635827Z",
     "start_time": "2019-05-30T12:33:29.842168Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 228ms/step\n",
      "0 [D loss: 0.897402, acc.: 17.19%] [G loss: 0.459961]\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1 [D loss: 0.389872, acc.: 67.19%] [G loss: 0.465183]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "2 [D loss: 0.420553, acc.: 65.62%] [G loss: 0.471371]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "3 [D loss: 0.508456, acc.: 60.94%] [G loss: 0.617696]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "4 [D loss: 0.598696, acc.: 64.06%] [G loss: 0.924843]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "5 [D loss: 0.439067, acc.: 68.75%] [G loss: 1.369893]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "6 [D loss: 0.305575, acc.: 79.69%] [G loss: 2.044326]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "7 [D loss: 0.114698, acc.: 98.44%] [G loss: 3.027683]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "8 [D loss: 0.046196, acc.: 100.00%] [G loss: 3.794942]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "9 [D loss: 0.052363, acc.: 98.44%] [G loss: 4.734261]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "10 [D loss: 0.018306, acc.: 100.00%] [G loss: 5.057650]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "11 [D loss: 0.082395, acc.: 98.44%] [G loss: 5.120262]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "12 [D loss: 0.027824, acc.: 100.00%] [G loss: 4.754051]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "13 [D loss: 0.016725, acc.: 100.00%] [G loss: 5.002081]\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "14 [D loss: 0.052593, acc.: 98.44%] [G loss: 4.611884]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "15 [D loss: 0.080419, acc.: 93.75%] [G loss: 4.361791]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "16 [D loss: 0.112647, acc.: 95.31%] [G loss: 4.857446]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "17 [D loss: 0.110861, acc.: 95.31%] [G loss: 4.999882]\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "18 [D loss: 0.119274, acc.: 93.75%] [G loss: 5.634763]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "19 [D loss: 0.023383, acc.: 100.00%] [G loss: 6.207585]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "20 [D loss: 0.080156, acc.: 96.88%] [G loss: 6.558522]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "21 [D loss: 0.009560, acc.: 100.00%] [G loss: 7.114886]\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "22 [D loss: 0.033343, acc.: 98.44%] [G loss: 6.844009]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "23 [D loss: 0.011585, acc.: 100.00%] [G loss: 7.653965]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "24 [D loss: 0.030312, acc.: 98.44%] [G loss: 7.772991]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "25 [D loss: 0.008674, acc.: 100.00%] [G loss: 7.830296]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "26 [D loss: 0.016222, acc.: 98.44%] [G loss: 8.469939]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "27 [D loss: 0.014117, acc.: 100.00%] [G loss: 8.393036]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "28 [D loss: 0.008004, acc.: 100.00%] [G loss: 8.328285]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "29 [D loss: 0.004671, acc.: 100.00%] [G loss: 8.541874]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "30 [D loss: 0.001196, acc.: 100.00%] [G loss: 8.255856]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "31 [D loss: 0.016185, acc.: 98.44%] [G loss: 8.303508]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "32 [D loss: 0.013018, acc.: 100.00%] [G loss: 7.964654]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "33 [D loss: 0.013780, acc.: 100.00%] [G loss: 7.656186]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "34 [D loss: 0.009760, acc.: 100.00%] [G loss: 7.531384]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "35 [D loss: 0.068181, acc.: 98.44%] [G loss: 7.319972]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "36 [D loss: 0.023492, acc.: 98.44%] [G loss: 7.013154]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "37 [D loss: 0.027774, acc.: 98.44%] [G loss: 7.309729]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "38 [D loss: 0.034699, acc.: 100.00%] [G loss: 6.956170]\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "39 [D loss: 0.039552, acc.: 98.44%] [G loss: 6.833642]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "40 [D loss: 0.019948, acc.: 100.00%] [G loss: 7.205578]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "41 [D loss: 0.007548, acc.: 100.00%] [G loss: 7.147226]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "42 [D loss: 0.024511, acc.: 100.00%] [G loss: 7.252687]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "43 [D loss: 0.005836, acc.: 100.00%] [G loss: 7.303045]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "44 [D loss: 0.036129, acc.: 98.44%] [G loss: 6.712781]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "45 [D loss: 0.045661, acc.: 98.44%] [G loss: 6.756023]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "46 [D loss: 0.011162, acc.: 100.00%] [G loss: 6.890597]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "47 [D loss: 0.016400, acc.: 100.00%] [G loss: 6.943652]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "48 [D loss: 0.010206, acc.: 100.00%] [G loss: 7.400043]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "49 [D loss: 0.019944, acc.: 98.44%] [G loss: 7.902189]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "50 [D loss: 0.011628, acc.: 100.00%] [G loss: 8.244727]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "51 [D loss: 0.005744, acc.: 100.00%] [G loss: 7.744656]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "52 [D loss: 0.013626, acc.: 100.00%] [G loss: 7.469518]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "53 [D loss: 0.021733, acc.: 100.00%] [G loss: 7.011386]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "54 [D loss: 0.025369, acc.: 98.44%] [G loss: 7.668778]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "55 [D loss: 0.005225, acc.: 100.00%] [G loss: 8.247165]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "56 [D loss: 0.019854, acc.: 100.00%] [G loss: 7.919830]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "57 [D loss: 0.016285, acc.: 100.00%] [G loss: 7.175156]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "58 [D loss: 0.006620, acc.: 100.00%] [G loss: 7.730394]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "59 [D loss: 0.030577, acc.: 98.44%] [G loss: 7.422580]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "60 [D loss: 0.010872, acc.: 100.00%] [G loss: 7.325789]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "61 [D loss: 0.017080, acc.: 98.44%] [G loss: 7.772881]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "62 [D loss: 0.043377, acc.: 98.44%] [G loss: 6.858005]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "63 [D loss: 0.055781, acc.: 98.44%] [G loss: 7.905778]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "64 [D loss: 0.003018, acc.: 100.00%] [G loss: 9.464039]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "65 [D loss: 0.060521, acc.: 96.88%] [G loss: 9.147995]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "66 [D loss: 0.003448, acc.: 100.00%] [G loss: 8.868306]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "67 [D loss: 0.005638, acc.: 100.00%] [G loss: 9.006434]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "68 [D loss: 0.004321, acc.: 100.00%] [G loss: 8.381399]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "69 [D loss: 0.001346, acc.: 100.00%] [G loss: 8.274181]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "70 [D loss: 0.008384, acc.: 100.00%] [G loss: 8.347086]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "71 [D loss: 0.002453, acc.: 100.00%] [G loss: 7.780649]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "72 [D loss: 0.009391, acc.: 100.00%] [G loss: 8.319117]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "73 [D loss: 0.010792, acc.: 100.00%] [G loss: 7.087704]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "74 [D loss: 0.035072, acc.: 98.44%] [G loss: 6.427374]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "75 [D loss: 0.010761, acc.: 100.00%] [G loss: 6.707622]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "76 [D loss: 0.066133, acc.: 98.44%] [G loss: 7.280013]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "77 [D loss: 0.024391, acc.: 100.00%] [G loss: 9.056351]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "78 [D loss: 0.012413, acc.: 100.00%] [G loss: 9.405357]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "79 [D loss: 0.010478, acc.: 100.00%] [G loss: 9.669462]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "80 [D loss: 0.014632, acc.: 98.44%] [G loss: 9.689231]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "81 [D loss: 0.000631, acc.: 100.00%] [G loss: 10.056086]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "82 [D loss: 0.000692, acc.: 100.00%] [G loss: 9.395445]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "83 [D loss: 0.008771, acc.: 100.00%] [G loss: 8.794477]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "84 [D loss: 0.046041, acc.: 98.44%] [G loss: 8.890185]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "85 [D loss: 0.017627, acc.: 100.00%] [G loss: 9.133050]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "86 [D loss: 0.033241, acc.: 98.44%] [G loss: 8.549953]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "87 [D loss: 0.042058, acc.: 96.88%] [G loss: 10.445831]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "88 [D loss: 0.017366, acc.: 100.00%] [G loss: 11.653652]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "89 [D loss: 0.000294, acc.: 100.00%] [G loss: 14.328516]\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "90 [D loss: 0.036570, acc.: 98.44%] [G loss: 15.401392]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "91 [D loss: 0.013673, acc.: 100.00%] [G loss: 14.408205]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "92 [D loss: 0.001952, acc.: 100.00%] [G loss: 13.400538]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "93 [D loss: 0.001111, acc.: 100.00%] [G loss: 13.786174]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "94 [D loss: 0.010411, acc.: 100.00%] [G loss: 12.056292]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "95 [D loss: 0.060980, acc.: 95.31%] [G loss: 13.976998]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "96 [D loss: 0.047959, acc.: 98.44%] [G loss: 14.859451]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "97 [D loss: 0.001203, acc.: 100.00%] [G loss: 16.496347]\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "98 [D loss: 0.001752, acc.: 100.00%] [G loss: 17.319803]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "99 [D loss: 0.011544, acc.: 100.00%] [G loss: 17.536095]\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "100 [D loss: 0.000357, acc.: 100.00%] [G loss: 16.530075]\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "101 [D loss: 0.004508, acc.: 100.00%] [G loss: 14.632412]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "102 [D loss: 0.004583, acc.: 100.00%] [G loss: 11.958269]\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "103 [D loss: 0.002752, acc.: 100.00%] [G loss: 9.821049]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "104 [D loss: 0.156542, acc.: 92.19%] [G loss: 12.393340]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "105 [D loss: 0.001438, acc.: 100.00%] [G loss: 17.411453]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "106 [D loss: 0.281429, acc.: 89.06%] [G loss: 10.149973]\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "107 [D loss: 0.150348, acc.: 95.31%] [G loss: 6.217104]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "108 [D loss: 0.236503, acc.: 93.75%] [G loss: 12.421112]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "109 [D loss: 0.000057, acc.: 100.00%] [G loss: 21.253719]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "110 [D loss: 0.009517, acc.: 100.00%] [G loss: 30.200630]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "111 [D loss: 0.096945, acc.: 96.88%] [G loss: 35.046368]\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "112 [D loss: 0.000000, acc.: 100.00%] [G loss: 43.226261]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "113 [D loss: 0.000000, acc.: 100.00%] [G loss: 44.142906]\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "114 [D loss: 0.000000, acc.: 100.00%] [G loss: 44.849079]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "115 [D loss: 0.000000, acc.: 100.00%] [G loss: 41.211906]\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "116 [D loss: 0.036633, acc.: 98.44%] [G loss: 40.703159]\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "117 [D loss: 0.000000, acc.: 100.00%] [G loss: 37.421890]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "118 [D loss: 0.000008, acc.: 100.00%] [G loss: 31.579533]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "119 [D loss: 0.002777, acc.: 100.00%] [G loss: 28.129236]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "120 [D loss: 0.014380, acc.: 98.44%] [G loss: 24.287149]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "121 [D loss: 0.272299, acc.: 90.62%] [G loss: 22.193872]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "122 [D loss: 0.748185, acc.: 81.25%] [G loss: 18.401510]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "123 [D loss: 0.230790, acc.: 93.75%] [G loss: 22.877411]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "124 [D loss: 0.027244, acc.: 98.44%] [G loss: 27.163136]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "125 [D loss: 0.228732, acc.: 85.94%] [G loss: 20.140844]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "126 [D loss: 0.620467, acc.: 92.19%] [G loss: 17.319715]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "127 [D loss: 0.176681, acc.: 95.31%] [G loss: 19.212299]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "128 [D loss: 0.000308, acc.: 100.00%] [G loss: 22.817707]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "129 [D loss: 0.008134, acc.: 100.00%] [G loss: 26.481770]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "130 [D loss: 0.144172, acc.: 95.31%] [G loss: 31.097549]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "131 [D loss: 0.000683, acc.: 100.00%] [G loss: 34.012505]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "132 [D loss: 0.004402, acc.: 100.00%] [G loss: 35.618561]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "133 [D loss: 0.003690, acc.: 100.00%] [G loss: 35.635582]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "134 [D loss: 0.064382, acc.: 96.88%] [G loss: 31.160980]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "135 [D loss: 0.175887, acc.: 98.44%] [G loss: 25.279076]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "136 [D loss: 0.023153, acc.: 98.44%] [G loss: 24.077349]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "137 [D loss: 0.002023, acc.: 100.00%] [G loss: 22.101170]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "138 [D loss: 0.454210, acc.: 90.62%] [G loss: 19.726202]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "139 [D loss: 0.222790, acc.: 95.31%] [G loss: 22.432098]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "140 [D loss: 0.002821, acc.: 100.00%] [G loss: 25.471308]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "141 [D loss: 0.174597, acc.: 96.88%] [G loss: 22.217007]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "142 [D loss: 0.062514, acc.: 98.44%] [G loss: 20.846893]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "143 [D loss: 0.015697, acc.: 100.00%] [G loss: 19.123962]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "144 [D loss: 0.009215, acc.: 100.00%] [G loss: 19.242828]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "145 [D loss: 0.002346, acc.: 100.00%] [G loss: 17.648960]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "146 [D loss: 0.001929, acc.: 100.00%] [G loss: 17.071404]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "147 [D loss: 0.103352, acc.: 95.31%] [G loss: 16.658655]\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "148 [D loss: 0.083769, acc.: 96.88%] [G loss: 13.402165]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "149 [D loss: 0.079617, acc.: 96.88%] [G loss: 9.235790]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "150 [D loss: 0.285369, acc.: 92.19%] [G loss: 12.967633]\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "151 [D loss: 0.000366, acc.: 100.00%] [G loss: 20.856808]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "152 [D loss: 0.019943, acc.: 98.44%] [G loss: 25.656425]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "153 [D loss: 0.043431, acc.: 98.44%] [G loss: 24.293804]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "154 [D loss: 0.000386, acc.: 100.00%] [G loss: 24.197247]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "155 [D loss: 0.274827, acc.: 95.31%] [G loss: 23.109402]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "156 [D loss: 0.000242, acc.: 100.00%] [G loss: 25.502609]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "157 [D loss: 0.000163, acc.: 100.00%] [G loss: 26.834538]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "158 [D loss: 0.000287, acc.: 100.00%] [G loss: 24.585310]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "159 [D loss: 0.044333, acc.: 98.44%] [G loss: 20.101944]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "160 [D loss: 0.004569, acc.: 100.00%] [G loss: 15.785432]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "161 [D loss: 0.041276, acc.: 96.88%] [G loss: 15.975632]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "162 [D loss: 0.001421, acc.: 100.00%] [G loss: 14.315256]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "163 [D loss: 0.014719, acc.: 98.44%] [G loss: 15.661913]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "164 [D loss: 0.010376, acc.: 100.00%] [G loss: 15.074564]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "165 [D loss: 0.034634, acc.: 100.00%] [G loss: 9.872049]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "166 [D loss: 0.330364, acc.: 92.19%] [G loss: 11.051730]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "167 [D loss: 0.000779, acc.: 100.00%] [G loss: 16.930025]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "168 [D loss: 0.171576, acc.: 90.62%] [G loss: 9.170835]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "169 [D loss: 0.252563, acc.: 92.19%] [G loss: 8.517312]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "170 [D loss: 0.536315, acc.: 90.62%] [G loss: 16.141214]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "171 [D loss: 0.000539, acc.: 100.00%] [G loss: 23.592447]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "172 [D loss: 0.000324, acc.: 100.00%] [G loss: 33.036758]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "173 [D loss: 0.007328, acc.: 100.00%] [G loss: 35.849499]\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "174 [D loss: 0.147942, acc.: 95.31%] [G loss: 34.249779]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "175 [D loss: 0.000482, acc.: 100.00%] [G loss: 32.562580]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "176 [D loss: 0.000038, acc.: 100.00%] [G loss: 29.345757]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "177 [D loss: 0.100316, acc.: 98.44%] [G loss: 25.686522]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "178 [D loss: 0.036028, acc.: 98.44%] [G loss: 26.108984]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "179 [D loss: 0.109428, acc.: 96.88%] [G loss: 23.712265]\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "180 [D loss: 0.000213, acc.: 100.00%] [G loss: 25.029572]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "181 [D loss: 0.005413, acc.: 100.00%] [G loss: 25.391163]\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "182 [D loss: 0.214993, acc.: 92.19%] [G loss: 13.337977]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "183 [D loss: 0.941350, acc.: 84.38%] [G loss: 10.786742]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "184 [D loss: 0.003864, acc.: 100.00%] [G loss: 16.861652]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "185 [D loss: 0.002510, acc.: 100.00%] [G loss: 19.639063]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "186 [D loss: 0.042950, acc.: 98.44%] [G loss: 20.117443]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "187 [D loss: 0.245142, acc.: 87.50%] [G loss: 14.892046]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "188 [D loss: 0.032777, acc.: 98.44%] [G loss: 12.292233]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "189 [D loss: 0.025171, acc.: 100.00%] [G loss: 10.673041]\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "190 [D loss: 0.047305, acc.: 96.88%] [G loss: 10.764141]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "191 [D loss: 0.001643, acc.: 100.00%] [G loss: 12.414055]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "192 [D loss: 0.007543, acc.: 100.00%] [G loss: 13.057174]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "193 [D loss: 0.053346, acc.: 98.44%] [G loss: 10.150454]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "194 [D loss: 0.053796, acc.: 98.44%] [G loss: 7.501923]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "195 [D loss: 0.168368, acc.: 96.88%] [G loss: 7.353863]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "196 [D loss: 0.025847, acc.: 100.00%] [G loss: 9.490786]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "197 [D loss: 0.049242, acc.: 100.00%] [G loss: 9.160955]\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "198 [D loss: 0.075550, acc.: 98.44%] [G loss: 7.902530]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "199 [D loss: 0.114072, acc.: 96.88%] [G loss: 8.695801]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "200 [D loss: 0.046941, acc.: 98.44%] [G loss: 7.932393]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "201 [D loss: 0.086494, acc.: 98.44%] [G loss: 5.851297]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "202 [D loss: 0.096446, acc.: 96.88%] [G loss: 7.822433]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "203 [D loss: 0.042865, acc.: 98.44%] [G loss: 10.854244]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "204 [D loss: 0.301546, acc.: 89.06%] [G loss: 7.325584]\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "205 [D loss: 0.342945, acc.: 90.62%] [G loss: 9.180543]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "206 [D loss: 0.001170, acc.: 100.00%] [G loss: 16.149734]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "207 [D loss: 0.217365, acc.: 90.62%] [G loss: 12.455488]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "208 [D loss: 0.604199, acc.: 81.25%] [G loss: 11.545010]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "209 [D loss: 0.002035, acc.: 100.00%] [G loss: 19.617712]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "210 [D loss: 0.327280, acc.: 85.94%] [G loss: 15.347301]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "211 [D loss: 0.308335, acc.: 89.06%] [G loss: 12.009240]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "212 [D loss: 0.063215, acc.: 96.88%] [G loss: 16.768524]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "213 [D loss: 0.015950, acc.: 100.00%] [G loss: 19.245117]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "214 [D loss: 0.165298, acc.: 92.19%] [G loss: 13.643834]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "215 [D loss: 0.013616, acc.: 98.44%] [G loss: 12.418240]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "216 [D loss: 0.080019, acc.: 96.88%] [G loss: 10.155743]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "217 [D loss: 0.063977, acc.: 96.88%] [G loss: 10.931344]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "218 [D loss: 0.001098, acc.: 100.00%] [G loss: 12.894888]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "219 [D loss: 0.025514, acc.: 100.00%] [G loss: 12.901530]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "220 [D loss: 0.062734, acc.: 98.44%] [G loss: 9.333055]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "221 [D loss: 0.039770, acc.: 100.00%] [G loss: 7.309780]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "222 [D loss: 0.117017, acc.: 93.75%] [G loss: 7.761673]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "223 [D loss: 0.002915, acc.: 100.00%] [G loss: 10.564270]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "224 [D loss: 0.075046, acc.: 98.44%] [G loss: 9.196736]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "225 [D loss: 0.029687, acc.: 100.00%] [G loss: 7.478059]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "226 [D loss: 0.092930, acc.: 96.88%] [G loss: 8.142042]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "227 [D loss: 0.003701, acc.: 100.00%] [G loss: 11.617306]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "228 [D loss: 0.018100, acc.: 100.00%] [G loss: 12.322989]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "229 [D loss: 0.020155, acc.: 100.00%] [G loss: 10.419661]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "230 [D loss: 0.020525, acc.: 100.00%] [G loss: 9.631466]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "231 [D loss: 0.048145, acc.: 98.44%] [G loss: 10.661074]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "232 [D loss: 0.003655, acc.: 100.00%] [G loss: 12.908755]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "233 [D loss: 0.042064, acc.: 100.00%] [G loss: 10.984639]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "234 [D loss: 0.069373, acc.: 98.44%] [G loss: 10.411057]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "235 [D loss: 0.043223, acc.: 98.44%] [G loss: 11.201542]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "236 [D loss: 0.090721, acc.: 96.88%] [G loss: 11.519138]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "237 [D loss: 0.039648, acc.: 100.00%] [G loss: 9.479351]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "238 [D loss: 0.003302, acc.: 100.00%] [G loss: 8.621830]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "239 [D loss: 0.131584, acc.: 96.88%] [G loss: 10.082518]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "240 [D loss: 0.006281, acc.: 100.00%] [G loss: 14.627518]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "241 [D loss: 1.523966, acc.: 42.19%] [G loss: 6.550083]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "242 [D loss: 0.583050, acc.: 85.94%] [G loss: 13.706820]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "243 [D loss: 0.001046, acc.: 100.00%] [G loss: 24.611250]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "244 [D loss: 0.307679, acc.: 89.06%] [G loss: 25.041765]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "245 [D loss: 0.044520, acc.: 98.44%] [G loss: 22.482822]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "246 [D loss: 0.042173, acc.: 98.44%] [G loss: 21.135216]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "247 [D loss: 0.059231, acc.: 98.44%] [G loss: 19.921688]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "248 [D loss: 0.003974, acc.: 100.00%] [G loss: 20.791225]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "249 [D loss: 0.032672, acc.: 96.88%] [G loss: 21.571117]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "250 [D loss: 0.005652, acc.: 100.00%] [G loss: 19.435656]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "251 [D loss: 0.002166, acc.: 100.00%] [G loss: 22.271761]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "252 [D loss: 0.000928, acc.: 100.00%] [G loss: 19.168926]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "253 [D loss: 0.002270, acc.: 100.00%] [G loss: 19.352871]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "254 [D loss: 0.003794, acc.: 100.00%] [G loss: 17.395266]\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "255 [D loss: 0.010461, acc.: 100.00%] [G loss: 16.105085]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "256 [D loss: 0.040032, acc.: 98.44%] [G loss: 13.028711]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "257 [D loss: 0.070596, acc.: 96.88%] [G loss: 10.401037]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "258 [D loss: 0.032832, acc.: 98.44%] [G loss: 9.555449]\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "259 [D loss: 0.062927, acc.: 96.88%] [G loss: 11.081656]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "260 [D loss: 0.044527, acc.: 98.44%] [G loss: 10.194794]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "261 [D loss: 0.066734, acc.: 98.44%] [G loss: 7.909898]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "262 [D loss: 0.047398, acc.: 98.44%] [G loss: 5.887626]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "263 [D loss: 0.020896, acc.: 100.00%] [G loss: 5.636918]\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "264 [D loss: 0.072008, acc.: 96.88%] [G loss: 7.044711]\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "265 [D loss: 0.012035, acc.: 100.00%] [G loss: 9.275560]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "266 [D loss: 0.035013, acc.: 100.00%] [G loss: 8.952271]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "267 [D loss: 0.027018, acc.: 100.00%] [G loss: 9.351396]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "268 [D loss: 0.007630, acc.: 100.00%] [G loss: 10.250528]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "269 [D loss: 0.003655, acc.: 100.00%] [G loss: 9.254641]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "270 [D loss: 0.005045, acc.: 100.00%] [G loss: 10.278193]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "271 [D loss: 0.008801, acc.: 100.00%] [G loss: 9.195888]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "272 [D loss: 0.030089, acc.: 98.44%] [G loss: 9.678261]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "273 [D loss: 0.007192, acc.: 100.00%] [G loss: 10.213158]\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "274 [D loss: 0.017799, acc.: 100.00%] [G loss: 9.549354]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "275 [D loss: 0.129901, acc.: 93.75%] [G loss: 10.749498]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "276 [D loss: 0.078549, acc.: 96.88%] [G loss: 9.355364]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "277 [D loss: 0.015452, acc.: 100.00%] [G loss: 7.178891]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "278 [D loss: 0.011829, acc.: 100.00%] [G loss: 6.615720]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "279 [D loss: 0.009875, acc.: 100.00%] [G loss: 7.051835]\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "280 [D loss: 0.049394, acc.: 96.88%] [G loss: 10.202002]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "281 [D loss: 0.018971, acc.: 100.00%] [G loss: 12.331638]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "282 [D loss: 0.135956, acc.: 90.62%] [G loss: 4.508823]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "283 [D loss: 0.699387, acc.: 78.12%] [G loss: 9.516067]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "284 [D loss: 0.001069, acc.: 100.00%] [G loss: 25.663330]\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "285 [D loss: 3.057165, acc.: 51.56%] [G loss: 4.359205]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "286 [D loss: 2.574753, acc.: 57.81%] [G loss: 2.238161]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "287 [D loss: 1.092972, acc.: 81.25%] [G loss: 7.634577]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "288 [D loss: 0.313241, acc.: 92.19%] [G loss: 18.893871]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "289 [D loss: 0.001486, acc.: 100.00%] [G loss: 31.594025]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "290 [D loss: 1.523376, acc.: 51.56%] [G loss: 23.263872]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "291 [D loss: 0.044240, acc.: 96.88%] [G loss: 15.823911]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "292 [D loss: 0.393073, acc.: 92.19%] [G loss: 12.749892]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "293 [D loss: 0.134343, acc.: 95.31%] [G loss: 14.812975]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "294 [D loss: 0.043486, acc.: 98.44%] [G loss: 13.253144]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "295 [D loss: 0.171229, acc.: 96.88%] [G loss: 13.462287]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "296 [D loss: 0.047309, acc.: 98.44%] [G loss: 13.537008]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "297 [D loss: 0.065772, acc.: 95.31%] [G loss: 15.071885]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "298 [D loss: 0.001037, acc.: 100.00%] [G loss: 15.814926]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "299 [D loss: 0.001467, acc.: 100.00%] [G loss: 16.986446]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "300 [D loss: 0.017745, acc.: 100.00%] [G loss: 16.603138]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "301 [D loss: 0.035141, acc.: 100.00%] [G loss: 14.541036]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "302 [D loss: 0.057307, acc.: 98.44%] [G loss: 12.192032]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "303 [D loss: 0.100243, acc.: 96.88%] [G loss: 10.173576]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "304 [D loss: 0.121331, acc.: 93.75%] [G loss: 10.296515]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "305 [D loss: 0.013993, acc.: 100.00%] [G loss: 10.312681]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "306 [D loss: 0.038188, acc.: 98.44%] [G loss: 9.399179]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "307 [D loss: 0.025588, acc.: 100.00%] [G loss: 9.026049]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "308 [D loss: 0.030218, acc.: 100.00%] [G loss: 8.611160]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "309 [D loss: 0.032341, acc.: 100.00%] [G loss: 8.077044]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "310 [D loss: 0.051005, acc.: 98.44%] [G loss: 7.788145]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "311 [D loss: 0.026739, acc.: 100.00%] [G loss: 6.881773]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "312 [D loss: 0.032796, acc.: 100.00%] [G loss: 6.391166]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "313 [D loss: 0.048491, acc.: 100.00%] [G loss: 6.125945]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "314 [D loss: 0.089417, acc.: 98.44%] [G loss: 5.090600]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "315 [D loss: 0.151199, acc.: 95.31%] [G loss: 6.078733]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "316 [D loss: 0.052298, acc.: 98.44%] [G loss: 8.707472]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "317 [D loss: 0.166055, acc.: 93.75%] [G loss: 5.024025]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "318 [D loss: 0.185439, acc.: 90.62%] [G loss: 5.659534]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "319 [D loss: 0.022770, acc.: 100.00%] [G loss: 7.024885]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "320 [D loss: 0.039790, acc.: 100.00%] [G loss: 7.794936]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "321 [D loss: 0.091388, acc.: 95.31%] [G loss: 5.996208]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "322 [D loss: 0.032483, acc.: 100.00%] [G loss: 5.231148]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "323 [D loss: 0.036593, acc.: 98.44%] [G loss: 5.234565]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "324 [D loss: 0.057152, acc.: 98.44%] [G loss: 5.904305]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "325 [D loss: 0.033760, acc.: 98.44%] [G loss: 7.436145]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "326 [D loss: 0.034211, acc.: 100.00%] [G loss: 7.901033]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "327 [D loss: 0.021881, acc.: 100.00%] [G loss: 7.440494]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "328 [D loss: 0.065856, acc.: 98.44%] [G loss: 5.821974]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "329 [D loss: 0.033418, acc.: 100.00%] [G loss: 5.855451]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "330 [D loss: 0.103233, acc.: 98.44%] [G loss: 6.634402]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "331 [D loss: 0.016384, acc.: 100.00%] [G loss: 8.585373]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "332 [D loss: 0.128500, acc.: 96.88%] [G loss: 5.220726]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "333 [D loss: 0.181396, acc.: 92.19%] [G loss: 5.536148]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "334 [D loss: 0.013147, acc.: 100.00%] [G loss: 8.596363]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "335 [D loss: 0.017916, acc.: 100.00%] [G loss: 10.797338]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "336 [D loss: 0.089588, acc.: 98.44%] [G loss: 9.066349]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "337 [D loss: 0.034613, acc.: 100.00%] [G loss: 6.411609]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "338 [D loss: 0.216041, acc.: 90.62%] [G loss: 8.254932]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "339 [D loss: 0.002918, acc.: 100.00%] [G loss: 12.941376]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "340 [D loss: 0.127590, acc.: 96.88%] [G loss: 10.142061]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "341 [D loss: 0.016758, acc.: 100.00%] [G loss: 7.645261]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "342 [D loss: 0.077990, acc.: 96.88%] [G loss: 6.820337]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "343 [D loss: 0.054837, acc.: 96.88%] [G loss: 8.400061]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "344 [D loss: 0.006305, acc.: 100.00%] [G loss: 10.372154]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "345 [D loss: 0.007917, acc.: 100.00%] [G loss: 12.213522]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "346 [D loss: 0.040951, acc.: 100.00%] [G loss: 11.193181]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "347 [D loss: 0.015743, acc.: 100.00%] [G loss: 9.089167]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "348 [D loss: 0.016391, acc.: 100.00%] [G loss: 6.752821]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "349 [D loss: 0.006719, acc.: 100.00%] [G loss: 6.797296]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "350 [D loss: 0.034160, acc.: 100.00%] [G loss: 6.617759]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "351 [D loss: 0.016883, acc.: 100.00%] [G loss: 8.417751]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "352 [D loss: 0.019987, acc.: 98.44%] [G loss: 9.491619]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "353 [D loss: 0.058282, acc.: 96.88%] [G loss: 7.426096]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "354 [D loss: 0.071190, acc.: 98.44%] [G loss: 6.327193]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "355 [D loss: 0.018590, acc.: 100.00%] [G loss: 6.582863]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "356 [D loss: 0.003365, acc.: 100.00%] [G loss: 8.049946]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "357 [D loss: 0.009160, acc.: 100.00%] [G loss: 8.391745]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "358 [D loss: 0.036811, acc.: 100.00%] [G loss: 7.035117]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "359 [D loss: 0.021745, acc.: 100.00%] [G loss: 6.090523]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "360 [D loss: 0.066634, acc.: 96.88%] [G loss: 6.746364]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "361 [D loss: 0.032564, acc.: 100.00%] [G loss: 7.614825]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "362 [D loss: 0.018719, acc.: 100.00%] [G loss: 7.299624]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "363 [D loss: 0.165816, acc.: 93.75%] [G loss: 7.655838]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "364 [D loss: 0.048299, acc.: 98.44%] [G loss: 6.684498]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "365 [D loss: 0.037366, acc.: 98.44%] [G loss: 5.665077]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "366 [D loss: 0.117069, acc.: 93.75%] [G loss: 6.151846]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "367 [D loss: 0.022548, acc.: 100.00%] [G loss: 8.559525]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "368 [D loss: 0.125149, acc.: 93.75%] [G loss: 6.325723]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "369 [D loss: 0.160548, acc.: 90.62%] [G loss: 6.626463]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "370 [D loss: 0.020103, acc.: 98.44%] [G loss: 10.594557]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "371 [D loss: 0.048871, acc.: 100.00%] [G loss: 11.144020]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "372 [D loss: 0.150932, acc.: 90.62%] [G loss: 12.136566]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "373 [D loss: 0.029329, acc.: 100.00%] [G loss: 13.601358]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "374 [D loss: 0.026028, acc.: 100.00%] [G loss: 13.554783]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "375 [D loss: 0.012180, acc.: 100.00%] [G loss: 12.554914]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "376 [D loss: 0.008057, acc.: 100.00%] [G loss: 10.778573]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "377 [D loss: 0.009073, acc.: 100.00%] [G loss: 8.935526]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "378 [D loss: 0.012958, acc.: 100.00%] [G loss: 8.342617]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "379 [D loss: 0.016542, acc.: 100.00%] [G loss: 7.632220]\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "380 [D loss: 0.020913, acc.: 98.44%] [G loss: 10.180401]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "381 [D loss: 0.004623, acc.: 100.00%] [G loss: 11.281846]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "382 [D loss: 0.056980, acc.: 98.44%] [G loss: 8.281284]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "383 [D loss: 0.015638, acc.: 100.00%] [G loss: 7.530212]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "384 [D loss: 0.036669, acc.: 100.00%] [G loss: 7.498122]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "385 [D loss: 0.013120, acc.: 100.00%] [G loss: 9.559317]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "386 [D loss: 0.024056, acc.: 100.00%] [G loss: 9.073820]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "387 [D loss: 0.079365, acc.: 95.31%] [G loss: 7.114833]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "388 [D loss: 0.060633, acc.: 96.88%] [G loss: 6.692637]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "389 [D loss: 0.029934, acc.: 100.00%] [G loss: 8.792623]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "390 [D loss: 0.052962, acc.: 98.44%] [G loss: 7.470611]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "391 [D loss: 0.065596, acc.: 96.88%] [G loss: 6.912741]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "392 [D loss: 0.082917, acc.: 98.44%] [G loss: 8.693977]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "393 [D loss: 0.047664, acc.: 98.44%] [G loss: 9.574397]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "394 [D loss: 0.107859, acc.: 98.44%] [G loss: 5.817394]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "395 [D loss: 0.026281, acc.: 100.00%] [G loss: 5.591058]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "396 [D loss: 0.045573, acc.: 98.44%] [G loss: 8.863992]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "397 [D loss: 0.015517, acc.: 100.00%] [G loss: 11.143633]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "398 [D loss: 0.102006, acc.: 96.88%] [G loss: 5.733881]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "399 [D loss: 0.202464, acc.: 90.62%] [G loss: 6.757603]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "400 [D loss: 0.003116, acc.: 100.00%] [G loss: 11.761406]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "401 [D loss: 0.086523, acc.: 96.88%] [G loss: 8.812561]\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "402 [D loss: 0.040983, acc.: 98.44%] [G loss: 6.008078]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "403 [D loss: 0.121412, acc.: 93.75%] [G loss: 8.305702]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "404 [D loss: 0.005211, acc.: 100.00%] [G loss: 12.241821]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "405 [D loss: 0.202765, acc.: 93.75%] [G loss: 5.386342]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "406 [D loss: 0.538036, acc.: 78.12%] [G loss: 7.130988]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "407 [D loss: 0.002714, acc.: 100.00%] [G loss: 15.861864]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "408 [D loss: 0.753821, acc.: 68.75%] [G loss: 2.125348]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "409 [D loss: 1.627377, acc.: 54.69%] [G loss: 1.778221]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "410 [D loss: 0.087042, acc.: 98.44%] [G loss: 10.414270]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "411 [D loss: 0.007715, acc.: 100.00%] [G loss: 21.435810]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "412 [D loss: 1.001732, acc.: 59.38%] [G loss: 10.616772]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "413 [D loss: 0.121058, acc.: 96.88%] [G loss: 4.887719]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "414 [D loss: 0.269116, acc.: 89.06%] [G loss: 2.908962]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "415 [D loss: 0.254189, acc.: 90.62%] [G loss: 5.460309]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "416 [D loss: 0.025704, acc.: 100.00%] [G loss: 9.357713]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "417 [D loss: 0.014366, acc.: 100.00%] [G loss: 13.200630]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "418 [D loss: 0.128202, acc.: 93.75%] [G loss: 13.497160]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "419 [D loss: 0.094990, acc.: 96.88%] [G loss: 11.032829]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "420 [D loss: 0.025667, acc.: 100.00%] [G loss: 8.432117]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "421 [D loss: 0.019142, acc.: 100.00%] [G loss: 6.331320]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "422 [D loss: 0.054045, acc.: 98.44%] [G loss: 5.205584]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "423 [D loss: 0.061609, acc.: 100.00%] [G loss: 6.285904]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "424 [D loss: 0.014398, acc.: 100.00%] [G loss: 7.520847]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "425 [D loss: 0.016212, acc.: 100.00%] [G loss: 8.672544]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "426 [D loss: 0.052046, acc.: 100.00%] [G loss: 8.757181]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "427 [D loss: 0.075903, acc.: 96.88%] [G loss: 6.093163]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "428 [D loss: 0.044185, acc.: 100.00%] [G loss: 4.207121]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "429 [D loss: 0.063471, acc.: 98.44%] [G loss: 4.474463]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "430 [D loss: 0.025234, acc.: 100.00%] [G loss: 5.799689]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "431 [D loss: 0.054289, acc.: 98.44%] [G loss: 6.872301]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "432 [D loss: 0.073724, acc.: 98.44%] [G loss: 6.077662]\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "433 [D loss: 0.037450, acc.: 100.00%] [G loss: 4.956974]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "434 [D loss: 0.069974, acc.: 96.88%] [G loss: 4.730364]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "435 [D loss: 0.039095, acc.: 100.00%] [G loss: 5.015849]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "436 [D loss: 0.038191, acc.: 100.00%] [G loss: 5.870683]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "437 [D loss: 0.129613, acc.: 93.75%] [G loss: 5.877838]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "438 [D loss: 0.018609, acc.: 100.00%] [G loss: 7.407304]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "439 [D loss: 0.073213, acc.: 98.44%] [G loss: 5.883981]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "440 [D loss: 0.038815, acc.: 100.00%] [G loss: 4.730087]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "441 [D loss: 0.221459, acc.: 92.19%] [G loss: 6.276483]\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "442 [D loss: 0.034861, acc.: 98.44%] [G loss: 10.479300]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "443 [D loss: 0.234393, acc.: 85.94%] [G loss: 4.033698]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "444 [D loss: 0.521449, acc.: 81.25%] [G loss: 5.629830]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "445 [D loss: 0.010413, acc.: 100.00%] [G loss: 12.240658]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "446 [D loss: 0.105605, acc.: 96.88%] [G loss: 13.919437]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "447 [D loss: 0.333400, acc.: 87.50%] [G loss: 6.265092]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "448 [D loss: 0.160607, acc.: 93.75%] [G loss: 3.831268]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "449 [D loss: 0.122341, acc.: 95.31%] [G loss: 5.978845]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "450 [D loss: 0.084470, acc.: 98.44%] [G loss: 10.009148]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "451 [D loss: 0.049348, acc.: 98.44%] [G loss: 12.624114]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "452 [D loss: 0.146082, acc.: 93.75%] [G loss: 10.107412]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "453 [D loss: 0.024442, acc.: 100.00%] [G loss: 6.942526]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "454 [D loss: 0.037367, acc.: 98.44%] [G loss: 5.169766]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "455 [D loss: 0.035217, acc.: 98.44%] [G loss: 5.562434]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "456 [D loss: 0.011300, acc.: 100.00%] [G loss: 5.923068]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "457 [D loss: 0.034032, acc.: 100.00%] [G loss: 7.121583]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "458 [D loss: 0.011432, acc.: 100.00%] [G loss: 7.957022]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "459 [D loss: 0.054018, acc.: 98.44%] [G loss: 7.588016]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "460 [D loss: 0.014912, acc.: 100.00%] [G loss: 6.149721]\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "461 [D loss: 0.021673, acc.: 100.00%] [G loss: 6.381572]\n"
     ]
    }
   ],
   "source": [
    "d_loss_hist = []\n",
    "g_loss_hist = []\n",
    "d_acc = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Discriminator\n",
    "    # ---------------------\n",
    "    \n",
    "    # Pick 50% of sample images\n",
    "    idx = np.random.randint(0, X_train_sc.shape[0], half_batch)\n",
    "    imgs = X_train_sc[idx]\n",
    "\n",
    "    # Generate 50% of new images\n",
    "    noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "    gen_imgs = generator_model.predict(noise)\n",
    "    \n",
    "    \n",
    "    # Train discriminator on real images with label 1\n",
    "    d_loss_real = discriminator_model.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "    \n",
    "    # Train discriminator on fake images with label 0\n",
    "    d_loss_fake = discriminator_model.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "    \n",
    "    # Loss of discriminator = Mean of Real and Fake loss\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "    d_loss_hist.append(d_loss[0])\n",
    "    d_acc.append(d_loss[1])\n",
    "    \n",
    "    # ---------------------\n",
    "    #  Train Generator\n",
    "    # ---------------------\n",
    "\n",
    "    # The generator wants the discriminator to label the generated samples as valid (ones)\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "    valid_y = np.array([1] * batch_size)\n",
    "\n",
    "    # Train the generator\n",
    "    g_loss = GAN_model.train_on_batch(noise, valid_y)\n",
    "    g_loss_hist.append(g_loss)\n",
    "    \n",
    "    # Print the progress\n",
    "    print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "        \n",
    "    if epoch % save_interval == 0:\n",
    "        save_imgs(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.171875,\n",
       " 0.671875,\n",
       " 0.65625,\n",
       " 0.609375,\n",
       " 0.640625,\n",
       " 0.6875,\n",
       " 0.796875,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.9375,\n",
       " 0.953125,\n",
       " 0.953125,\n",
       " 0.9375,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.953125,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.921875,\n",
       " 1.0,\n",
       " 0.890625,\n",
       " 0.953125,\n",
       " 0.9375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.90625,\n",
       " 0.8125,\n",
       " 0.9375,\n",
       " 0.984375,\n",
       " 0.859375,\n",
       " 0.921875,\n",
       " 0.953125,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.953125,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.90625,\n",
       " 0.953125,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.953125,\n",
       " 0.96875,\n",
       " 0.96875,\n",
       " 0.921875,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.953125,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.921875,\n",
       " 1.0,\n",
       " 0.90625,\n",
       " 0.921875,\n",
       " 0.90625,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.953125,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.921875,\n",
       " 0.84375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.875,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 0.890625,\n",
       " 0.90625,\n",
       " 1.0,\n",
       " 0.90625,\n",
       " 0.8125,\n",
       " 1.0,\n",
       " 0.859375,\n",
       " 0.890625,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.921875,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.9375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.421875,\n",
       " 0.859375,\n",
       " 1.0,\n",
       " 0.890625,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9375,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.90625,\n",
       " 0.78125,\n",
       " 1.0,\n",
       " 0.515625,\n",
       " 0.578125,\n",
       " 0.8125,\n",
       " 0.921875,\n",
       " 1.0,\n",
       " 0.515625,\n",
       " 0.96875,\n",
       " 0.921875,\n",
       " 0.953125,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 0.953125,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 0.9375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.953125,\n",
       " 0.984375,\n",
       " 0.9375,\n",
       " 0.90625,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.953125,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.921875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.90625,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9375,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.9375,\n",
       " 1.0,\n",
       " 0.9375,\n",
       " 0.90625,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.90625,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.953125,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.90625,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 0.9375,\n",
       " 1.0,\n",
       " 0.9375,\n",
       " 0.78125,\n",
       " 1.0,\n",
       " 0.6875,\n",
       " 0.546875,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.59375,\n",
       " 0.96875,\n",
       " 0.890625,\n",
       " 0.90625,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9375,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.921875,\n",
       " 0.984375,\n",
       " 0.859375,\n",
       " 0.8125,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.875,\n",
       " 0.9375,\n",
       " 0.953125,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.9375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.703125,\n",
       " 0.984375,\n",
       " 0.328125,\n",
       " 0.515625,\n",
       " 0.609375,\n",
       " 1.0,\n",
       " 0.59375,\n",
       " 0.921875,\n",
       " 0.96875,\n",
       " 0.9375,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.953125,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.9375,\n",
       " 1.0,\n",
       " 0.953125,\n",
       " 0.890625,\n",
       " 0.984375,\n",
       " 0.828125,\n",
       " 0.84375,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.9375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.953125,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.953125,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.875,\n",
       " 1.0,\n",
       " 0.703125,\n",
       " 0.59375,\n",
       " 0.953125,\n",
       " 1.0,\n",
       " 0.578125,\n",
       " 0.859375,\n",
       " 0.890625,\n",
       " 0.9375,\n",
       " 1.0,\n",
       " 0.890625,\n",
       " 0.84375,\n",
       " 0.9375,\n",
       " 0.890625,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 0.953125,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 0.9375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.953125,\n",
       " 0.984375,\n",
       " 0.953125,\n",
       " 0.953125,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.953125,\n",
       " 0.8125,\n",
       " 1.0,\n",
       " 0.578125,\n",
       " 0.546875,\n",
       " 0.546875,\n",
       " 0.625,\n",
       " 1.0,\n",
       " 0.875,\n",
       " 0.65625,\n",
       " 0.921875,\n",
       " 0.921875,\n",
       " 0.921875,\n",
       " 0.890625,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.953125,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.921875,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 0.953125,\n",
       " 0.921875,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.953125,\n",
       " 0.875,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.90625,\n",
       " 0.875,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.875,\n",
       " 0.953125,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.9375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.921875,\n",
       " 0.828125,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.515625,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.765625,\n",
       " 0.984375,\n",
       " 0.9375,\n",
       " 0.9375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.953125,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 0.921875,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.953125,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.921875,\n",
       " 1.0,\n",
       " 0.9375,\n",
       " 0.796875,\n",
       " 1.0,\n",
       " 0.640625,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.890625,\n",
       " 0.71875,\n",
       " 0.984375,\n",
       " 0.8125,\n",
       " 0.890625,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.9375,\n",
       " 0.921875,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 0.921875,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.953125,\n",
       " 0.9375,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9375,\n",
       " 0.8125,\n",
       " 1.0,\n",
       " 0.828125,\n",
       " 0.796875,\n",
       " 0.9375,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.921875,\n",
       " 0.890625,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.96875,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.953125,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 0.984375,\n",
       " 1.0,\n",
       " 0.984375,\n",
       " ...]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8974015116691589,\n",
       " 0.38987156830276604,\n",
       " 0.4205533863023103,\n",
       " 0.5084556341184887,\n",
       " 0.59869647026088,\n",
       " 0.4390666782860944,\n",
       " 0.3055751919754561,\n",
       " 0.11469815687794843,\n",
       " 0.046196417473233,\n",
       " 0.05236280451208586,\n",
       " 0.018305685836821795,\n",
       " 0.08239517733454704,\n",
       " 0.027823796477264295,\n",
       " 0.016725404664361287,\n",
       " 0.052593413777812965,\n",
       " 0.08041871339094392,\n",
       " 0.1126473173499337,\n",
       " 0.11086121946573568,\n",
       " 0.11927367746830013,\n",
       " 0.023383095860482178,\n",
       " 0.08015586435794968,\n",
       " 0.009559979662301528,\n",
       " 0.03334311395927474,\n",
       " 0.011585191825078715,\n",
       " 0.030312245723545274,\n",
       " 0.008673673540306043,\n",
       " 0.016222152969216468,\n",
       " 0.014117174274964617,\n",
       " 0.00800360839457781,\n",
       " 0.0046708852716363936,\n",
       " 0.0011961459285458886,\n",
       " 0.016185389087695512,\n",
       " 0.013018244004342705,\n",
       " 0.013780346547719091,\n",
       " 0.00975957722403109,\n",
       " 0.06818064325489104,\n",
       " 0.023492386186262593,\n",
       " 0.027773667534347624,\n",
       " 0.03469947166740894,\n",
       " 0.03955163201317191,\n",
       " 0.019948102009948343,\n",
       " 0.007547845292720012,\n",
       " 0.024510936142178252,\n",
       " 0.005836119642481208,\n",
       " 0.03612903784960508,\n",
       " 0.04566077965137083,\n",
       " 0.0111624935693726,\n",
       " 0.016399687736793567,\n",
       " 0.010205578408204019,\n",
       " 0.01994437212124467,\n",
       " 0.011627583298832178,\n",
       " 0.005743515212088823,\n",
       " 0.013625535182654858,\n",
       " 0.021732569177402183,\n",
       " 0.02536861399858026,\n",
       " 0.00522513993200846,\n",
       " 0.019854300655424595,\n",
       " 0.01628468045964837,\n",
       " 0.006619950901949778,\n",
       " 0.030577305878978223,\n",
       " 0.010872228536754847,\n",
       " 0.01707989606074989,\n",
       " 0.04337742365896702,\n",
       " 0.05578148446511477,\n",
       " 0.003017714072484523,\n",
       " 0.060520678758621216,\n",
       " 0.003448321484029293,\n",
       " 0.005637770240355167,\n",
       " 0.0043212393575231545,\n",
       " 0.001345803673757473,\n",
       " 0.008383633336052299,\n",
       " 0.0024526838678866625,\n",
       " 0.009391418541781604,\n",
       " 0.010791800916194916,\n",
       " 0.03507222980260849,\n",
       " 0.01076140534132719,\n",
       " 0.06613258412107825,\n",
       " 0.02439104951918125,\n",
       " 0.012413367629051208,\n",
       " 0.010477841948159039,\n",
       " 0.014632065605837852,\n",
       " 0.0006312380064628087,\n",
       " 0.0006918791332282126,\n",
       " 0.008771107997745275,\n",
       " 0.04604135773843154,\n",
       " 0.017626775428652763,\n",
       " 0.033241428434848785,\n",
       " 0.04205814964370802,\n",
       " 0.017365757252264302,\n",
       " 0.00029377423197729513,\n",
       " 0.03657033294439316,\n",
       " 0.013673477340489626,\n",
       " 0.0019515573512762785,\n",
       " 0.0011111081512353849,\n",
       " 0.010410605999936706,\n",
       " 0.060979769708865206,\n",
       " 0.04795944015495479,\n",
       " 0.0012026242820866173,\n",
       " 0.0017524864415463526,\n",
       " 0.011544419452548027,\n",
       " 0.0003574668662622571,\n",
       " 0.004508108249865472,\n",
       " 0.004583260178151249,\n",
       " 0.0027520418698259164,\n",
       " 0.15654197014646343,\n",
       " 0.0014378577179741114,\n",
       " 0.2814288903027773,\n",
       " 0.15034815670502136,\n",
       " 0.23650346696376884,\n",
       " 5.6907923863070446e-05,\n",
       " 0.009517116472125062,\n",
       " 0.09694454073905945,\n",
       " 2.0069317606839395e-08,\n",
       " 5.514292797761841e-10,\n",
       " 2.2807278242444073e-10,\n",
       " 1.3689846343631297e-10,\n",
       " 0.03663294020011384,\n",
       " 1.3655855040511922e-08,\n",
       " 8.135651434315605e-06,\n",
       " 0.0027765994393575966,\n",
       " 0.014380475622601807,\n",
       " 0.27229855954647064,\n",
       " 0.7481846213340759,\n",
       " 0.23078958392591176,\n",
       " 0.027244314784184098,\n",
       " 0.228732124902308,\n",
       " 0.6204672466392962,\n",
       " 0.1766809523109245,\n",
       " 0.0003078442537258049,\n",
       " 0.008134302690474094,\n",
       " 0.14417205856542614,\n",
       " 0.000683268368447898,\n",
       " 0.004402265069074929,\n",
       " 0.003690300862103868,\n",
       " 0.06438173895003274,\n",
       " 0.17588707809045445,\n",
       " 0.02315321038804541,\n",
       " 0.0020226840933901258,\n",
       " 0.4542103558778763,\n",
       " 0.2227896235258413,\n",
       " 0.002821293102897471,\n",
       " 0.17459691219846718,\n",
       " 0.0625140520860441,\n",
       " 0.015696840899181552,\n",
       " 0.009215041252900846,\n",
       " 0.002346339519135654,\n",
       " 0.001928807352669537,\n",
       " 0.10335228405892849,\n",
       " 0.08376938104629517,\n",
       " 0.07961652055382729,\n",
       " 0.285369173012441,\n",
       " 0.00036557715793605894,\n",
       " 0.019943485190196952,\n",
       " 0.04343147155668703,\n",
       " 0.0003862474477500655,\n",
       " 0.27482745456200064,\n",
       " 0.00024170749929908197,\n",
       " 0.00016301745199598372,\n",
       " 0.000287265691781613,\n",
       " 0.04433313512708992,\n",
       " 0.004569434706354514,\n",
       " 0.041276483330875635,\n",
       " 0.0014214485854608938,\n",
       " 0.014719367725774646,\n",
       " 0.010376125224865973,\n",
       " 0.03463414229918271,\n",
       " 0.33036359109974,\n",
       " 0.0007785463094478473,\n",
       " 0.1715756137855351,\n",
       " 0.2525630701911723,\n",
       " 0.5363150971592177,\n",
       " 0.0005394778947902523,\n",
       " 0.00032385427948611323,\n",
       " 0.007328138570301235,\n",
       " 0.14794185908976942,\n",
       " 0.0004824892967008054,\n",
       " 3.837880164780927e-05,\n",
       " 0.10031620155859855,\n",
       " 0.03602774684804899,\n",
       " 0.10942833558601706,\n",
       " 0.00021300675143720582,\n",
       " 0.005412726498207121,\n",
       " 0.2149933871696703,\n",
       " 0.9413501264989463,\n",
       " 0.003864057487589889,\n",
       " 0.0025095799937844276,\n",
       " 0.04294964059954509,\n",
       " 0.24514201283454895,\n",
       " 0.032776793581433594,\n",
       " 0.025170614244416356,\n",
       " 0.04730534757254645,\n",
       " 0.0016425263893324882,\n",
       " 0.007543176296167076,\n",
       " 0.05334599292837083,\n",
       " 0.053796134889125824,\n",
       " 0.16836809751112014,\n",
       " 0.02584741171449423,\n",
       " 0.04924161918461323,\n",
       " 0.07555009797215462,\n",
       " 0.11407216265797615,\n",
       " 0.046940939500927925,\n",
       " 0.08649377524852753,\n",
       " 0.09644602320622653,\n",
       " 0.04286464536562562,\n",
       " 0.30154556035995483,\n",
       " 0.3429452313139336,\n",
       " 0.001169772178400308,\n",
       " 0.2173645633738488,\n",
       " 0.6041994150727987,\n",
       " 0.0020352075443952344,\n",
       " 0.32727951370179653,\n",
       " 0.30833460204303265,\n",
       " 0.0632153939222917,\n",
       " 0.015950409753713757,\n",
       " 0.16529833665117621,\n",
       " 0.013615507632493973,\n",
       " 0.08001912909094244,\n",
       " 0.06397715985076502,\n",
       " 0.001097542466595769,\n",
       " 0.02551379142096266,\n",
       " 0.06273445075203199,\n",
       " 0.03977006487548351,\n",
       " 0.11701685408479534,\n",
       " 0.0029148993780836463,\n",
       " 0.07504562754184008,\n",
       " 0.029687448404729366,\n",
       " 0.0929301893338561,\n",
       " 0.003700721834320575,\n",
       " 0.018099509441526607,\n",
       " 0.02015465815202333,\n",
       " 0.02052528876811266,\n",
       " 0.04814483970403671,\n",
       " 0.0036551840603351593,\n",
       " 0.04206411133054644,\n",
       " 0.06937266141176224,\n",
       " 0.0432226643897593,\n",
       " 0.09072142094373703,\n",
       " 0.03964784648269415,\n",
       " 0.0033016338711604476,\n",
       " 0.13158371625468135,\n",
       " 0.006280706013058079,\n",
       " 1.5239661931991577,\n",
       " 0.5830500001393375,\n",
       " 0.0010464556398801506,\n",
       " 0.3076794921080932,\n",
       " 0.044519927445435314,\n",
       " 0.04217330552637577,\n",
       " 0.05923086608527228,\n",
       " 0.0039742493245285004,\n",
       " 0.032672356319380924,\n",
       " 0.005651723069604486,\n",
       " 0.002165595651604235,\n",
       " 0.0009276243799831718,\n",
       " 0.002269833523314446,\n",
       " 0.0037935773143544793,\n",
       " 0.010460731042257976,\n",
       " 0.040032371412962675,\n",
       " 0.07059621065855026,\n",
       " 0.03283165767788887,\n",
       " 0.06292738020420074,\n",
       " 0.044526830257382244,\n",
       " 0.06673422828316689,\n",
       " 0.047398355789482594,\n",
       " 0.02089591044932604,\n",
       " 0.0720083573833108,\n",
       " 0.01203525741584599,\n",
       " 0.03501303493976593,\n",
       " 0.027017900720238686,\n",
       " 0.0076304092071950436,\n",
       " 0.0036549284122884274,\n",
       " 0.005045282654464245,\n",
       " 0.008800737094134092,\n",
       " 0.030089070554822683,\n",
       " 0.007192075252532959,\n",
       " 0.017798685934394598,\n",
       " 0.12990064918994904,\n",
       " 0.07854922348633409,\n",
       " 0.015451924875378609,\n",
       " 0.011828916845843196,\n",
       " 0.009875452058622614,\n",
       " 0.04939353885129094,\n",
       " 0.01897135889157653,\n",
       " 0.13595645688474178,\n",
       " 0.6993868627178017,\n",
       " 0.0010691338832202746,\n",
       " 3.0571645231248112,\n",
       " 2.574752982298378,\n",
       " 1.0929716863033718,\n",
       " 0.3132410860289383,\n",
       " 0.0014864776271057423,\n",
       " 1.5233761848940048,\n",
       " 0.04423973383381963,\n",
       " 0.3930731861910317,\n",
       " 0.13434268849960063,\n",
       " 0.043486074446263956,\n",
       " 0.17122871945912266,\n",
       " 0.047309438181400765,\n",
       " 0.065772237139754,\n",
       " 0.0010371578391641378,\n",
       " 0.0014667478403680434,\n",
       " 0.01774525883820388,\n",
       " 0.03514119801548077,\n",
       " 0.05730728137132246,\n",
       " 0.1002426128834486,\n",
       " 0.12133056833408773,\n",
       " 0.013992999214679003,\n",
       " 0.03818804258480668,\n",
       " 0.025587856769561768,\n",
       " 0.030217882245779037,\n",
       " 0.03234102111309767,\n",
       " 0.051005471497774124,\n",
       " 0.02673913538455963,\n",
       " 0.03279590606689453,\n",
       " 0.04849117808043957,\n",
       " 0.08941740915179253,\n",
       " 0.15119915269315243,\n",
       " 0.052297646179795265,\n",
       " 0.16605521412566304,\n",
       " 0.1854391172528267,\n",
       " 0.022769571281969547,\n",
       " 0.03979020030237734,\n",
       " 0.09138797223567963,\n",
       " 0.03248330391943455,\n",
       " 0.03659338876605034,\n",
       " 0.05715234484523535,\n",
       " 0.03375960048288107,\n",
       " 0.03421145584434271,\n",
       " 0.021881283493712544,\n",
       " 0.06585624255239964,\n",
       " 0.03341837786138058,\n",
       " 0.10323298536241055,\n",
       " 0.01638449029996991,\n",
       " 0.12849970697425306,\n",
       " 0.18139616772532463,\n",
       " 0.01314716343767941,\n",
       " 0.017915605450980365,\n",
       " 0.08958849677583203,\n",
       " 0.03461282700300217,\n",
       " 0.21604081708937883,\n",
       " 0.002917849546065554,\n",
       " 0.12759027280844748,\n",
       " 0.016758314799517393,\n",
       " 0.07798984227702022,\n",
       " 0.054837302915984765,\n",
       " 0.0063046058639883995,\n",
       " 0.007917026581708342,\n",
       " 0.04095087471068837,\n",
       " 0.015743420139187947,\n",
       " 0.016391284530982375,\n",
       " 0.006719496799632907,\n",
       " 0.03415988490451127,\n",
       " 0.01688347361050546,\n",
       " 0.019986736588180065,\n",
       " 0.05828238418325782,\n",
       " 0.071190245449543,\n",
       " 0.018590204184874892,\n",
       " 0.0033648894168436527,\n",
       " 0.009160087094642222,\n",
       " 0.03681123070418835,\n",
       " 0.02174530364573002,\n",
       " 0.06663355091586709,\n",
       " 0.03256439510732889,\n",
       " 0.018718623905442655,\n",
       " 0.16581645794212818,\n",
       " 0.048299066722393036,\n",
       " 0.03736634273082018,\n",
       " 0.1170693812891841,\n",
       " 0.0225477390922606,\n",
       " 0.12514865025877953,\n",
       " 0.16054791025817394,\n",
       " 0.02010287408484146,\n",
       " 0.04887086059898138,\n",
       " 0.15093225240707397,\n",
       " 0.02932853577658534,\n",
       " 0.02602804124398972,\n",
       " 0.012179530342109501,\n",
       " 0.00805678276810795,\n",
       " 0.009072811575606465,\n",
       " 0.012958447099663317,\n",
       " 0.01654235876048915,\n",
       " 0.020912794454488903,\n",
       " 0.004623171407729387,\n",
       " 0.0569795707706362,\n",
       " 0.015637789852917194,\n",
       " 0.03666906710714102,\n",
       " 0.01311956625431776,\n",
       " 0.02405604417435825,\n",
       " 0.07936524972319603,\n",
       " 0.06063304841518402,\n",
       " 0.029933802783489227,\n",
       " 0.05296185985207558,\n",
       " 0.0655955821275711,\n",
       " 0.0829169899225235,\n",
       " 0.04766397597268224,\n",
       " 0.1078585609793663,\n",
       " 0.026281297206878662,\n",
       " 0.045572737231850624,\n",
       " 0.015517350781010464,\n",
       " 0.10200609068851918,\n",
       " 0.2024644841440022,\n",
       " 0.003115594619885087,\n",
       " 0.0865230904892087,\n",
       " 0.04098280519247055,\n",
       " 0.12141201226040721,\n",
       " 0.005211210926063359,\n",
       " 0.20276527386158705,\n",
       " 0.5380355168599635,\n",
       " 0.002713785142987035,\n",
       " 0.7538208067417145,\n",
       " 1.6273771819251124,\n",
       " 0.08704232619493268,\n",
       " 0.007714978739386424,\n",
       " 1.0017316070443485,\n",
       " 0.12105800537392497,\n",
       " 0.2691160406684503,\n",
       " 0.2541885330574587,\n",
       " 0.02570418722461909,\n",
       " 0.014366406481713057,\n",
       " 0.128202067942766,\n",
       " 0.09499000740470365,\n",
       " 0.02566669648513198,\n",
       " 0.01914233434945345,\n",
       " 0.05404463270679116,\n",
       " 0.06160852545872331,\n",
       " 0.014398470986634493,\n",
       " 0.016212490969337523,\n",
       " 0.05204595159739256,\n",
       " 0.07590255048125982,\n",
       " 0.04418534226715565,\n",
       " 0.06347147887572646,\n",
       " 0.025234384462237358,\n",
       " 0.05428910814225674,\n",
       " 0.07372419675812125,\n",
       " 0.03744951402768493,\n",
       " 0.06997442804276943,\n",
       " 0.039095086976885796,\n",
       " 0.03819106472656131,\n",
       " 0.12961283326148987,\n",
       " 0.018609334947541356,\n",
       " 0.07321323500946164,\n",
       " 0.038815202191472054,\n",
       " 0.22145933285355568,\n",
       " 0.0348612281086389,\n",
       " 0.23439261876046658,\n",
       " 0.5214494569227099,\n",
       " 0.010412873700261116,\n",
       " 0.10560507377704198,\n",
       " 0.3334001589100808,\n",
       " 0.16060673166066408,\n",
       " 0.12234084855299443,\n",
       " 0.08446954376995564,\n",
       " 0.04934827025863342,\n",
       " 0.1460820835782215,\n",
       " 0.024442254099994898,\n",
       " 0.0373671711422503,\n",
       " 0.03521721623837948,\n",
       " 0.0113001917488873,\n",
       " 0.03403165005147457,\n",
       " 0.011432208586484194,\n",
       " 0.054018497467041016,\n",
       " 0.014911864884197712,\n",
       " 0.02167272660881281,\n",
       " 0.051113005727529526,\n",
       " 0.04204775020480156,\n",
       " 0.04207347333431244,\n",
       " 0.05803368426859379,\n",
       " 0.04256156738847494,\n",
       " 0.06482147984206676,\n",
       " 0.07691078167408705,\n",
       " 0.028474300168454647,\n",
       " 0.054357847198843956,\n",
       " 0.11689416691660881,\n",
       " 0.020800463389605284,\n",
       " 0.05319337313994765,\n",
       " 0.0735294846817851,\n",
       " 0.03211439959704876,\n",
       " 0.027304475661367178,\n",
       " 0.05039534065872431,\n",
       " 0.07574882172048092,\n",
       " 0.03259320370852947,\n",
       " 0.022192700998857617,\n",
       " 0.05607990827411413,\n",
       " 0.04645400866866112,\n",
       " 0.01446170685812831,\n",
       " 0.015859484672546387,\n",
       " 0.008486984763294458,\n",
       " 0.052223616279661655,\n",
       " 0.015309534966945648,\n",
       " 0.03089893516153097,\n",
       " 0.017598222009837627,\n",
       " 0.009915018337778747,\n",
       " 0.027871150989085436,\n",
       " 0.013014784548431635,\n",
       " 0.03263993235304952,\n",
       " 0.00834923516958952,\n",
       " 0.008068752940744162,\n",
       " 0.01996326819062233,\n",
       " 0.03582240082323551,\n",
       " 0.01276001613587141,\n",
       " 0.031455110758543015,\n",
       " 0.012110755313187838,\n",
       " 0.02340387273579836,\n",
       " 0.013471037382259965,\n",
       " 0.029977542930282652,\n",
       " 0.03200245462357998,\n",
       " 0.011799734318628907,\n",
       " 0.018403097987174988,\n",
       " 0.014917890774086118,\n",
       " 0.014885262120515108,\n",
       " 0.017662950325757265,\n",
       " 0.011241362430155277,\n",
       " 0.011193292564712465,\n",
       " 0.035429077222943306,\n",
       " 0.024838312529027462,\n",
       " 0.00954161211848259,\n",
       " 0.02099123690277338,\n",
       " 0.0211441561114043,\n",
       " 0.01004196098074317,\n",
       " 0.024523744359612465,\n",
       " 0.006280788220465183,\n",
       " 0.014930456411093473,\n",
       " 0.007730318233370781,\n",
       " 0.005876199807971716,\n",
       " 0.008829622995108366,\n",
       " 0.009628360159695148,\n",
       " 0.064431456848979,\n",
       " 0.009419051290024072,\n",
       " 0.7528400421142578,\n",
       " 0.04251022567405016,\n",
       " 3.2952753007411957,\n",
       " 3.7200546580752416,\n",
       " 1.51155010225375,\n",
       " 0.0011612888702074997,\n",
       " 1.540733354002441,\n",
       " 0.20944215910276398,\n",
       " 0.07649833522737026,\n",
       " 0.10384808271192014,\n",
       " 0.03426953963935375,\n",
       " 0.07074779272079468,\n",
       " 0.013028868474066257,\n",
       " 0.006598664680495858,\n",
       " 0.00942915421910584,\n",
       " 0.014808613574132323,\n",
       " 0.039883680641651154,\n",
       " 0.033413276076316833,\n",
       " 0.09282706677913666,\n",
       " 0.04356650449335575,\n",
       " 0.03586652223020792,\n",
       " 0.04124415130354464,\n",
       " 0.02052705781534314,\n",
       " 0.025481740944087505,\n",
       " 0.061444140039384365,\n",
       " 0.1042263563722372,\n",
       " 0.07864852249622345,\n",
       " 0.04052755422890186,\n",
       " 0.08487511426210403,\n",
       " 0.11300025135278702,\n",
       " 0.05643029324710369,\n",
       " 0.0452650785446167,\n",
       " 0.05875968001782894,\n",
       " 0.058502063155174255,\n",
       " 0.09721450135111809,\n",
       " 0.032553560100495815,\n",
       " 0.06963943596929312,\n",
       " 0.05583740584552288,\n",
       " 0.08232664689421654,\n",
       " 0.02913328818976879,\n",
       " 0.02340768463909626,\n",
       " 0.03234238550066948,\n",
       " 0.06260078586637974,\n",
       " 0.06037869397550821,\n",
       " 0.024184126406908035,\n",
       " 0.03594432771205902,\n",
       " 0.03644525445997715,\n",
       " 0.05466952547430992,\n",
       " 0.10473719146102667,\n",
       " 0.021960168844088912,\n",
       " 0.16515682730823755,\n",
       " 0.31376372277736664,\n",
       " 0.0417925079818815,\n",
       " 0.3649936690926552,\n",
       " 0.2913235411979258,\n",
       " 0.05550301959738135,\n",
       " 0.01365184030146338,\n",
       " 0.15767049154965207,\n",
       " 0.02371671423316002,\n",
       " 0.028922642581164837,\n",
       " 0.040684538427740335,\n",
       " 0.08175165834836662,\n",
       " 0.005710650235414505,\n",
       " 0.018123170535545796,\n",
       " 0.0376501502469182,\n",
       " 0.02951768576167524,\n",
       " 0.0202848669141531,\n",
       " 0.07644624076783657,\n",
       " 0.009925160673446953,\n",
       " 0.026741971261799335,\n",
       " 0.021270493045449257,\n",
       " 0.0223317863419652,\n",
       " 0.04757429985329509,\n",
       " 0.01253015547990799,\n",
       " 0.01886902656406164,\n",
       " 0.053349558264017105,\n",
       " 0.02080494910478592,\n",
       " 0.022083329036831856,\n",
       " 0.0393526554107666,\n",
       " 0.05802087299525738,\n",
       " 0.024865026585757732,\n",
       " 0.031478272285312414,\n",
       " 0.06876236759126186,\n",
       " 0.2936530648730695,\n",
       " 0.019405631348490715,\n",
       " 0.9298739731311798,\n",
       " 1.4654795587994158,\n",
       " 0.09803137509152293,\n",
       " 0.06329028108302737,\n",
       " 1.228308793681208,\n",
       " 0.32196700293570757,\n",
       " 0.2908733024960384,\n",
       " 0.20494533388409764,\n",
       " 0.015982290380634367,\n",
       " 0.21221605176106095,\n",
       " 0.29309260938316584,\n",
       " 0.10484804213047028,\n",
       " 0.30614791018888354,\n",
       " 0.10177962144371122,\n",
       " 0.010684138396754861,\n",
       " 0.010223690303973854,\n",
       " 0.06523573864251375,\n",
       " 0.03926720574963838,\n",
       " 0.05178600084036589,\n",
       " 0.028920911252498627,\n",
       " 0.03630193928256631,\n",
       " 0.03347988659515977,\n",
       " 0.01212399359792471,\n",
       " 0.007976004853844643,\n",
       " 0.02210264839231968,\n",
       " 0.03458241932094097,\n",
       " 0.019000517204403877,\n",
       " 0.023939273320138454,\n",
       " 0.039895061403512955,\n",
       " 0.02024245820939541,\n",
       " 0.026343603152781725,\n",
       " 0.03311621956527233,\n",
       " 0.032813785597682,\n",
       " 0.03553672879934311,\n",
       " 0.03674424346536398,\n",
       " 0.039217546582221985,\n",
       " 0.03371594101190567,\n",
       " 0.04379425570368767,\n",
       " 0.02133547980338335,\n",
       " 0.041650114580988884,\n",
       " 0.061940938234329224,\n",
       " 0.03106674086302519,\n",
       " 0.041096240282058716,\n",
       " 0.0271691526286304,\n",
       " 0.04949041921645403,\n",
       " 0.04244033060967922,\n",
       " 0.05837614741176367,\n",
       " 0.03054933436214924,\n",
       " 0.034419008530676365,\n",
       " 0.03296761680394411,\n",
       " 0.06764213368296623,\n",
       " 0.10742820799350739,\n",
       " 0.05114329606294632,\n",
       " 0.08100239839404821,\n",
       " 0.09578961413353682,\n",
       " 0.13258044235408306,\n",
       " 0.020920981653034687,\n",
       " 0.07687088241800666,\n",
       " 0.04004628723487258,\n",
       " 0.09957231720909476,\n",
       " 0.011784637754317373,\n",
       " 0.02446874079760164,\n",
       " 0.022457589744590223,\n",
       " 0.030433442792855203,\n",
       " 0.03463539946824312,\n",
       " 0.0208785654976964,\n",
       " 0.018469804897904396,\n",
       " 0.011908716522157192,\n",
       " 0.024532655254006386,\n",
       " 0.03535115160048008,\n",
       " 0.03256984055042267,\n",
       " 0.025904387468472123,\n",
       " 0.02186928689479828,\n",
       " 0.02598010888323188,\n",
       " 0.026436048559844494,\n",
       " 0.055054862052202225,\n",
       " 0.033150557428598404,\n",
       " 0.01693870034068823,\n",
       " 0.06428070552647114,\n",
       " 0.016437410376966,\n",
       " 0.037733156234025955,\n",
       " 0.044065726455301046,\n",
       " 0.028233895311132073,\n",
       " 0.025072339922189713,\n",
       " 0.03298047557473183,\n",
       " 0.10711362212896347,\n",
       " 0.015134521760046482,\n",
       " 0.03908295650035143,\n",
       " 0.016900100279599428,\n",
       " 0.019460231997072697,\n",
       " 0.013975149719044566,\n",
       " 0.019148631719872355,\n",
       " 0.01848052255809307,\n",
       " 0.013883161824196577,\n",
       " 0.032712473068386316,\n",
       " 0.018325572134926915,\n",
       " 0.048604599898681045,\n",
       " 0.07370857568457723,\n",
       " 0.018808378488756716,\n",
       " 0.10470354557037354,\n",
       " 0.11826910101808608,\n",
       " 0.003927334968466312,\n",
       " 0.03444325120653957,\n",
       " 0.025040664710104465,\n",
       " 0.053818621672689915,\n",
       " 0.009978494374081492,\n",
       " 0.1010301411151886,\n",
       " 0.031332953833043575,\n",
       " 0.06862927007023245,\n",
       " 0.08307847287505865,\n",
       " 0.02563490718603134,\n",
       " 0.021938043646514416,\n",
       " 0.009246853180229664,\n",
       " 0.010134558659046888,\n",
       " 0.01557215943466872,\n",
       " 0.018537054769694805,\n",
       " 0.03616925235837698,\n",
       " 0.013194221770390868,\n",
       " 0.056341541931033134,\n",
       " 0.007978268433362246,\n",
       " 0.015631402609869838,\n",
       " 0.023139920085668564,\n",
       " 0.008019547443836927,\n",
       " 0.015067055821418762,\n",
       " 0.050333716440945864,\n",
       " 0.02318135555833578,\n",
       " 0.03529232949949801,\n",
       " 0.04356419714167714,\n",
       " 0.014098045416176319,\n",
       " 0.05289682745933533,\n",
       " 0.14848590642213821,\n",
       " 0.33076308766612783,\n",
       " 0.04660600075840193,\n",
       " 3.558101162314415,\n",
       " 4.00132842606763,\n",
       " 4.4500345584311845,\n",
       " 2.1195353917446482,\n",
       " 0.030079395975917578,\n",
       " 0.34679707283794414,\n",
       " 1.1444017770700157,\n",
       " 0.22743982262909412,\n",
       " 0.1959819858893752,\n",
       " 0.16228302684612572,\n",
       " 0.1790691651403904,\n",
       " 0.021530442871153355,\n",
       " 0.01582337892614305,\n",
       " 0.10244932922068983,\n",
       " 0.09984317794442177,\n",
       " 0.04853122867643833,\n",
       " 0.03722840640693903,\n",
       " 0.046808418817818165,\n",
       " 0.025585623923689127,\n",
       " 0.0230972059071064,\n",
       " 0.018586493097245693,\n",
       " 0.01856941357254982,\n",
       " 0.02914105635136366,\n",
       " 0.043987334705889225,\n",
       " 0.08716843649744987,\n",
       " 0.05266880244016647,\n",
       " 0.02769388072192669,\n",
       " 0.02410117443650961,\n",
       " 0.07000099308788776,\n",
       " 0.05210883729159832,\n",
       " 0.05948062799870968,\n",
       " 0.056006770581007004,\n",
       " 0.020815037190914154,\n",
       " 0.04929729620926082,\n",
       " 0.06879035849124193,\n",
       " 0.07844011671841145,\n",
       " 0.033666365779936314,\n",
       " 0.055054306983947754,\n",
       " 0.044750504195690155,\n",
       " 0.05330060329288244,\n",
       " 0.04898284189403057,\n",
       " 0.01929866522550583,\n",
       " 0.02419701498001814,\n",
       " 0.031015433371067047,\n",
       " 0.01968217082321644,\n",
       " 0.041713918559253216,\n",
       " 0.045653593726456165,\n",
       " 0.04689512215554714,\n",
       " 0.03730472922325134,\n",
       " 0.0358002595603466,\n",
       " 0.02685467153787613,\n",
       " 0.19531037658452988,\n",
       " 0.10959174763411283,\n",
       " 0.06181530840694904,\n",
       " 0.14722956344485283,\n",
       " 0.15355882048606873,\n",
       " 0.016714719124138355,\n",
       " 0.053357178694568574,\n",
       " 0.1801830306649208,\n",
       " 0.2446216845419258,\n",
       " 0.01053937105461955,\n",
       " 0.07782274882629281,\n",
       " 0.2053357665427029,\n",
       " 0.31851487047970295,\n",
       " 0.02041440363973379,\n",
       " 0.0610493115382269,\n",
       " 0.21601518196985126,\n",
       " 0.1126512112095952,\n",
       " 0.033423032611608505,\n",
       " 0.01486112317070365,\n",
       " 0.03408147860318422,\n",
       " 0.014966854825615883,\n",
       " 0.07339620869606733,\n",
       " 0.04193573817610741,\n",
       " 0.0376892127096653,\n",
       " 0.03307404927909374,\n",
       " 0.011661300901323557,\n",
       " 0.030537029611878097,\n",
       " 0.030629160348325968,\n",
       " 0.0489910077303648,\n",
       " 0.05567024275660515,\n",
       " 0.043866015039384365,\n",
       " 0.040515321772545576,\n",
       " 0.018785743042826653,\n",
       " 0.06275568343698978,\n",
       " 0.03637575823813677,\n",
       " 0.021757920272648335,\n",
       " 0.0290427403524518,\n",
       " 0.05586239695549011,\n",
       " 0.0818147361278534,\n",
       " 0.08132726419717073,\n",
       " 0.09663270600140095,\n",
       " 0.058655571192502975,\n",
       " 0.03719646483659744,\n",
       " 0.04876626469194889,\n",
       " 0.020413394318893552,\n",
       " 0.18569306284189224,\n",
       " 0.42973391618579626,\n",
       " 0.008558491070289165,\n",
       " 0.8812900390475988,\n",
       " 1.8465983616188169,\n",
       " 0.053898055804893374,\n",
       " 0.021109160385094583,\n",
       " 0.5270791052607819,\n",
       " 0.050901964074000716,\n",
       " 0.10253578703850508,\n",
       " 0.14970446610823274,\n",
       " 0.029941278276965022,\n",
       " 0.016897317953407764,\n",
       " 0.013560672872699797,\n",
       " 0.04272204980952665,\n",
       " 0.08033895754488185,\n",
       " 0.02321509551256895,\n",
       " 0.04879690334200859,\n",
       " 0.03090108372271061,\n",
       " 0.028943823650479317,\n",
       " 0.02952429000288248,\n",
       " 0.014573419000953436,\n",
       " 0.03561857342720032,\n",
       " 0.04086660500615835,\n",
       " 0.03281092643737793,\n",
       " 0.037225181236863136,\n",
       " 0.04709867760539055,\n",
       " 0.047696202993392944,\n",
       " 0.06417708471417427,\n",
       " 0.13375598937273026,\n",
       " 0.1331459805369377,\n",
       " 0.054005466401576996,\n",
       " 0.05796527350321412,\n",
       " 0.10211528651416302,\n",
       " 0.04344883747398853,\n",
       " 0.03711741045117378,\n",
       " 0.0792948342859745,\n",
       " 0.13930997252464294,\n",
       " 0.03724973276257515,\n",
       " 0.061972832307219505,\n",
       " 0.07649555243551731,\n",
       " 0.06576180085539818,\n",
       " 0.060175816528499126,\n",
       " 0.05749598704278469,\n",
       " 0.02614864706993103,\n",
       " 0.06871641241014004,\n",
       " 0.04712632950395346,\n",
       " 0.031370796263217926,\n",
       " 0.03648638725280762,\n",
       " 0.03752298839390278,\n",
       " 0.06701011955738068,\n",
       " 0.02617649082094431,\n",
       " 0.05244278162717819,\n",
       " 0.040984620340168476,\n",
       " 0.02327884826809168,\n",
       " 0.06580552645027637,\n",
       " 0.0340291578322649,\n",
       " 0.03330698423087597,\n",
       " 0.08956088311970234,\n",
       " 0.05283417087048292,\n",
       " 0.03325014375150204,\n",
       " 0.03039858117699623,\n",
       " 0.06456803157925606,\n",
       " 0.059597259387373924,\n",
       " 0.03980730939656496,\n",
       " 0.014750175178050995,\n",
       " 0.061464251950383186,\n",
       " 0.11598457954823971,\n",
       " 0.010783678386360407,\n",
       " 0.16114322189241648,\n",
       " 0.34984409157186747,\n",
       " 0.025833333522314206,\n",
       " 1.6239122711122036,\n",
       " 2.4949741005839314,\n",
       " 0.8729303942964179,\n",
       " 0.009115288499742746,\n",
       " 0.29714702752153244,\n",
       " 1.0179329769234755,\n",
       " 0.051576776430010796,\n",
       " 0.40678207017481327,\n",
       " 0.16847925080219284,\n",
       " 0.04910412256140262,\n",
       " 0.0069283836055547,\n",
       " 0.040742888348177075,\n",
       " 0.1223137987726659,\n",
       " 0.13616578238725197,\n",
       " 0.013472509512212127,\n",
       " 0.04154375195503235,\n",
       " 0.100867485627532,\n",
       " 0.09120413893833756,\n",
       " 0.009457210078835487,\n",
       " 0.012439899612218142,\n",
       " 0.04262527445098385,\n",
       " 0.034853253164328635,\n",
       " 0.031568773090839386,\n",
       " 0.026169375516474247,\n",
       " 0.11359494924545288,\n",
       " 0.042563081718981266,\n",
       " 0.05320197530090809,\n",
       " 0.05172963626682758,\n",
       " 0.043863543309271336,\n",
       " 0.06130004674196243,\n",
       " 0.048075820319354534,\n",
       " 0.04811937361955643,\n",
       " 0.10719428583979607,\n",
       " 0.07228194922208786,\n",
       " 0.15539953112602234,\n",
       " 0.09223089553415775,\n",
       " 0.05115239694714546,\n",
       " 0.07246966869570315,\n",
       " 0.1593088135123253,\n",
       " 0.12230580300092697,\n",
       " 0.029292994178831577,\n",
       " 0.07001929939724505,\n",
       " 0.04674591077491641,\n",
       " 0.0935879684984684,\n",
       " 0.054697923362255096,\n",
       " 0.02375856414437294,\n",
       " 0.03460832289420068,\n",
       " 0.036565036280080676,\n",
       " 0.05995697062462568,\n",
       " 0.0563194090500474,\n",
       " 0.027411237359046936,\n",
       " 0.07304457202553749,\n",
       " 0.06100344518199563,\n",
       " 0.06986079551279545,\n",
       " 0.03982340730726719,\n",
       " 0.033738070633262396,\n",
       " 0.16323912609368563,\n",
       " 0.31638378277421,\n",
       " 0.009406072611454874,\n",
       " 0.7122480003163218,\n",
       " 0.44388921558856964,\n",
       " 0.15714836353436112,\n",
       " 0.035768506248132326,\n",
       " 0.6283311714068986,\n",
       " 0.16355769895017147,\n",
       " 0.20432761637493968,\n",
       " 0.029440315440297127,\n",
       " 0.046234953217208385,\n",
       " 0.09576773206936195,\n",
       " 0.0717254396295175,\n",
       " 0.04415775649249554,\n",
       " 0.08558204444125295,\n",
       " 0.0360915195196867,\n",
       " 0.022634463384747505,\n",
       " 0.020439313258975744,\n",
       " 0.05201151920482516,\n",
       " 0.06862716563045979,\n",
       " 0.039093379862606525,\n",
       " 0.02972298115491867,\n",
       " 0.050316144712269306,\n",
       " 0.01697568316012621,\n",
       " 0.03314615460112691,\n",
       " 0.059974301140755415,\n",
       " 0.05817624181509018,\n",
       " 0.07010428514331579,\n",
       " 0.03038228116929531,\n",
       " 0.07214787229895592,\n",
       " ...]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Create new samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7**. We now have all the elements required to generate new samples. What are according to you :\n",
    "- the steps to generate new samples ?\n",
    "- the part of the network we re-use ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8**. You are now asked to generate and visualize new samples from the steps you defined above. Pay attention when plotting generated images to :\n",
    "- rescale the images between 0 and 1 (as done previously)\n",
    "- reshape the generated image to 28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:26:43.879292Z",
     "start_time": "2019-12-03T09:26:43.875424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    }
   ],
   "source": [
    "# TODO: Visualize a generated image\n",
    "\n",
    "num_samples =5\n",
    "random_input = np.random.normal(size=(num_samples, latent_dim))\n",
    "generated_images = generator_model.predict(random_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_images = 0.5 * generated_images + 0.5 \n",
    "generated_images = generated_images.reshape((num_samples, 28, 28))  \n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(1, num_samples, i + 1)\n",
    "    plt.imshow(generated_images[i], cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
